{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "004ea300-5511-49fc-900d-fb88bd684ea7",
   "metadata": {},
   "source": [
    "## Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a873f6-462e-4be9-9358-4c6c8c90f665",
   "metadata": {},
   "source": [
    "An ensemble technique in machine learning is a method that combines the predictions of multiple individual models to produce a more accurate and robust prediction than any single model. The idea behind ensemble techniques is to leverage the diversity of multiple models to reduce errors and improve overall performance. Ensembles are widely used in machine learning because they often lead to better predictive results compared to using a single model.\n",
    "\n",
    "There are several common ensemble techniques, including:\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating)**: In bagging, multiple instances of the same model are trained on different subsets of the training data (with replacement), and their predictions are then averaged or aggregated. The most well-known algorithm using bagging is Random Forest.\n",
    "\n",
    "2. **Boosting**: Boosting is an ensemble technique in which models are trained sequentially, with each subsequent model focusing on the examples that the previous models found difficult. Popular boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost.\n",
    "\n",
    "3. **Stacking**: Stacking involves training multiple different models (base models) and then training another model (meta-model or blender) on top of their predictions. This meta-model learns how to combine the predictions of the base models to make a final prediction.\n",
    "\n",
    "4. **Voting**: Voting ensembles combine the predictions of multiple models by allowing them to \"vote\" on the final prediction. There are two types of voting: hard voting, where each model's prediction is treated as a discrete vote, and soft voting, where models provide probability estimates, and the final prediction is based on weighted averages of these probabilities.\n",
    "\n",
    "Ensemble techniques are valuable because they can improve generalization, reduce overfitting, and handle complex relationships in data. They are particularly effective when individual models in the ensemble have different strengths and weaknesses, as the ensemble can compensate for these weaknesses and amplify the strengths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db5db75-f4ae-4e09-b4b2-6fe010a3b442",
   "metadata": {},
   "source": [
    "### Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432496fb-c584-4957-98c5-50d70881434a",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning for several important reasons:\n",
    "\n",
    "1. **Improved Accuracy**: One of the primary motivations for using ensemble techniques is that they often result in improved predictive accuracy. By combining the predictions of multiple models, ensembles can reduce bias and variance, leading to more robust and accurate predictions. This is especially beneficial when working with complex, noisy, or high-dimensional datasets.\n",
    "\n",
    "2. **Reduced Overfitting**: Ensembles can help reduce overfitting, which occurs when a model learns to memorize the training data rather than generalize from it. By combining multiple models that might overfit in different ways, ensembles can create a more balanced and generalizable final prediction.\n",
    "\n",
    "3. **Robustness**: Ensembles are more robust to outliers and noisy data. If one of the individual models in the ensemble makes a poor prediction due to noise or outlier data points, the other models can compensate for it, resulting in a more reliable overall prediction.\n",
    "\n",
    "4. **Model Stability**: Ensembles increase the stability of predictions. Minor variations in the training data or model parameters can lead to different predictions from individual models. Ensembles, by averaging or combining these predictions, tend to produce more consistent and stable results.\n",
    "\n",
    "5. **Handling Complex Relationships**: In some cases, complex relationships within the data may not be captured by a single model. Ensembles can capture different aspects of the data by using diverse base models, and the combination of these diverse perspectives can lead to a more accurate prediction.\n",
    "\n",
    "6. **Flexibility**: Ensemble techniques are flexible and can be applied to a wide range of machine learning algorithms, including decision trees, neural networks, support vector machines, and more. This versatility allows practitioners to choose the most suitable base models for their specific problem.\n",
    "\n",
    "7. **Model Interpretability**: Some ensemble methods, like Random Forests, provide feature importance scores, which can help in understanding the relative importance of different features in making predictions. This can be valuable for feature selection and model interpretability.\n",
    "\n",
    "8. **State-of-the-Art Performance**: In many machine learning competitions and real-world applications, ensemble techniques have been shown to achieve state-of-the-art performance and win competitions. They are often part of winning solutions in data science competitions like Kaggle.\n",
    "\n",
    "9. **Risk Mitigation**: Ensembles can help mitigate the risk associated with relying on a single model. If a single model performs poorly due to model selection or random initialization, the overall ensemble performance may still be reasonable.\n",
    "\n",
    "Overall, ensemble techniques are a powerful tool in the machine learning toolkit because they harness the wisdom of multiple models to improve predictive performance, reduce errors, and enhance the reliability and robustness of machine learning systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09f7ce-475e-4c6a-8e67-52618999d50d",
   "metadata": {},
   "source": [
    "## Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824baf18-ca78-4f06-8387-35b84d6230c0",
   "metadata": {},
   "source": [
    "Bagging, short for Bootstrap Aggregating, is an ensemble machine learning technique used to improve the accuracy and robustness of predictive models, particularly decision trees. It was introduced by Leo Breiman in the 1990s. Bagging is primarily applied in situations where the base model (often a decision tree) is prone to high variance and overfitting.\n",
    "\n",
    "Here's how bagging works:\n",
    "\n",
    "1. **Bootstrap Sampling**: Bagging starts by creating multiple subsets of the original training dataset through a process called bootstrap sampling. This involves randomly selecting samples from the training data with replacement, which means that some samples may be selected multiple times, while others may not be selected at all. Each subset is roughly the same size as the original dataset.\n",
    "\n",
    "2. **Model Training**: A separate base model (e.g., decision tree) is trained on each of these bootstrap samples. Since each sample is slightly different due to the random selection with replacement, each model will have some variations in the patterns it learns.\n",
    "\n",
    "3. **Parallel Processing**: The training of these base models can be done in parallel, making bagging computationally efficient.\n",
    "\n",
    "4. **Prediction Aggregation**: When making predictions on new or unseen data, bagging combines the predictions from all the base models to produce a final prediction. For regression tasks, this often involves averaging the predictions from all models, while for classification tasks, it may involve majority voting.\n",
    "\n",
    "The key idea behind bagging is that by training multiple models on slightly different subsets of the data and then aggregating their predictions, it reduces the variance and overfitting of individual models. This results in a more robust and accurate ensemble model.\n",
    "\n",
    "The most well-known algorithm that uses bagging is the Random Forest. In a Random Forest, the base models are decision trees, and bagging is used to create diverse decision trees. Each tree is grown using a random subset of features as well, further increasing diversity.\n",
    "\n",
    "Bagging is a powerful technique that can be applied to various machine learning algorithms, not just decision trees. It is particularly useful when dealing with noisy or complex datasets and helps in reducing the risk of overfitting, making it a valuable tool in the ensemble learning toolbox."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab16be2f-bab9-49d5-a8de-c27046e15d19",
   "metadata": {},
   "source": [
    "## Q4. What is boosting?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b93e70-f9a9-4aa9-9adb-817b8e6ad3bc",
   "metadata": {},
   "source": [
    "Boosting is an ensemble machine learning technique used to improve the performance of weak or base models by combining them into a strong model. Unlike bagging, where multiple models are trained independently and then their predictions are aggregated, boosting builds an ensemble iteratively, with each new model focusing on the examples that previous models found difficult to classify correctly. Boosting was introduced by Robert Schapire and Yoram Singer in the late 1980s.\n",
    "\n",
    "Here's how boosting typically works:\n",
    "\n",
    "1. **Initialization**: Boosting starts with an initial base model, often a simple one like a decision stump (a one-level decision tree) or a perceptron (a simple neural network).\n",
    "\n",
    "2. **Weighted Data**: Each example in the training dataset is assigned an initial weight. Initially, all weights are set equally, so each example has the same importance.\n",
    "\n",
    "3. **Model Training Iteration**: Boosting consists of a series of iterations. In each iteration:\n",
    "   - The base model is trained on the training dataset, but the training data is weighted so that the model focuses more on the examples that were previously misclassified or had higher errors.\n",
    "   - After training, the base model's performance is evaluated, and it is assigned a weight based on how well it performed. Better models receive higher weights.\n",
    "   - The weights of the training examples are updated to give more emphasis to the examples that the current model struggled with.\n",
    "\n",
    "4. **Ensemble Construction**: The ensemble is constructed by combining the predictions of all the base models, with each model's contribution weighted based on its performance during training.\n",
    "\n",
    "5. **Final Prediction**: The final prediction is made by aggregating the predictions of all the base models, typically using weighted voting for classification tasks or weighted averaging for regression tasks.\n",
    "\n",
    "Boosting algorithms, such as AdaBoost (Adaptive Boosting), Gradient Boosting, and XGBoost, differ in their strategies for adjusting the example weights and combining the models. These algorithms have been highly successful in various machine learning applications and competitions.\n",
    "\n",
    "Boosting is particularly effective when used with weak base models, and it focuses on improving the overall ensemble's performance by emphasizing examples that are challenging for the current ensemble. It can lead to a significant reduction in bias and variance, resulting in highly accurate and robust predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad35ca4-161f-44e6-adb6-5fb6b5958ea9",
   "metadata": {},
   "source": [
    "## Q5. What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7251bd53-c407-4480-abd0-c6716f1191f7",
   "metadata": {},
   "source": [
    "Ensemble techniques in machine learning offer several benefits that make them widely used and effective in improving predictive model performance. Here are some of the key advantages of using ensemble techniques:\n",
    "\n",
    "1. **Improved Accuracy**: Ensembles typically produce more accurate predictions than individual base models. By combining multiple models with potentially different strengths and weaknesses, ensembles reduce errors and enhance the overall predictive accuracy.\n",
    "\n",
    "2. **Reduced Overfitting**: Ensembles are effective at reducing overfitting, a common problem where models perform well on the training data but poorly on unseen data. By combining multiple models that might overfit in different ways, ensembles create a more robust and generalizable prediction.\n",
    "\n",
    "3. **Robustness**: Ensembles are more robust to outliers, noisy data, and random variations in the training data. If one base model makes an erroneous prediction due to noise or outliers, the other models can compensate for it, resulting in a more reliable overall prediction.\n",
    "\n",
    "4. **Stability**: Ensembles provide more stable predictions. Minor changes in the training data or model parameters can lead to different predictions from individual models, but the ensemble's prediction tends to be more consistent and stable.\n",
    "\n",
    "5. **Handling Complex Relationships**: Ensembles are capable of capturing complex relationships within the data by using diverse base models. The combination of these diverse perspectives can lead to a more accurate prediction, especially when dealing with intricate or nonlinear data patterns.\n",
    "\n",
    "6. **Flexibility**: Ensemble techniques can be applied to a wide range of machine learning algorithms, including decision trees, neural networks, support vector machines, and more. This flexibility allows practitioners to choose the most suitable base models for their specific problem.\n",
    "\n",
    "7. **Model Interpretability**: Some ensemble methods, like Random Forests, provide feature importance scores, which can help in understanding the relative importance of different features in making predictions. This can be valuable for feature selection and model interpretability.\n",
    "\n",
    "8. **State-of-the-Art Performance**: Ensembles have achieved state-of-the-art performance in many machine learning competitions and real-world applications. They are often part of winning solutions in data science competitions like Kaggle.\n",
    "\n",
    "9. **Risk Mitigation**: Ensembles help mitigate the risk associated with relying on a single model. If a single model performs poorly due to model selection or random initialization, the overall ensemble performance may still be reasonable.\n",
    "\n",
    "10. **Versatility**: Ensembles can be customized and tailored to specific problem domains and datasets. Different ensemble techniques, such as bagging, boosting, and stacking, provide a range of strategies for combining models, allowing practitioners to choose the most appropriate method.\n",
    "\n",
    "Overall, ensemble techniques are valuable tools in machine learning because they harness the collective intelligence of multiple models to improve predictive performance, reduce errors, and enhance the reliability and robustness of machine learning systems. They are particularly useful when working with challenging or complex datasets and have become a fundamental part of modern machine learning workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d10acc-3378-46a4-bdb0-3c95d0560d9c",
   "metadata": {},
   "source": [
    "## Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef00c8-e2ef-41af-a988-32822602a2ba",
   "metadata": {},
   "source": [
    "Ensemble techniques are powerful tools in machine learning that can significantly improve the performance of predictive models in many cases. However, whether ensemble techniques are always better than individual models depends on several factors, and there are situations where using an ensemble may not be the best choice. Here are some considerations:\n",
    "\n",
    "1. **Data Quality**: If you have a small, clean, and well-structured dataset, individual models might perform well on their own, and there may be little need for ensembles. Ensembles are often more beneficial when dealing with noisy or complex data.\n",
    "\n",
    "2. **Model Selection**: If you select a strong, well-tuned individual model that is highly suited to your problem, it may perform sufficiently well without the need for an ensemble. Ensembles are often used when working with weaker base models or when you want to combine the strengths of multiple models.\n",
    "\n",
    "3. **Computational Resources**: Ensembles can be computationally intensive, especially if you are training a large number of base models or using complex ensemble methods. In cases where computational resources are limited, it may be more practical to focus on improving a single model.\n",
    "\n",
    "4. **Interpretability**: Ensembles can be less interpretable than individual models. If interpretability is a critical requirement for your application, using a single, easily interpretable model might be preferred.\n",
    "\n",
    "5. **Model Complexity**: Ensembles can add complexity to your model pipeline, making it more challenging to implement, deploy, and maintain. Simplicity and ease of use can be important considerations in certain scenarios.\n",
    "\n",
    "6. **Risk Tolerance**: In some situations, you may be willing to accept the risk of a single model's failure, while in others, you may want to minimize that risk by using an ensemble. The choice depends on your risk tolerance and the potential consequences of model errors.\n",
    "\n",
    "7. **Ensemble Size**: The size of the ensemble matters. Using too few base models in an ensemble might not provide significant benefits, while using too many may lead to diminishing returns or increased computational costs.\n",
    "\n",
    "In summary, ensemble techniques are not always better than individual models. They are a valuable tool to consider when working on machine learning projects, but their suitability depends on the specific characteristics of your data, the choice of base models, your computational resources, interpretability requirements, and risk tolerance. It's essential to carefully evaluate whether an ensemble approach is justified and to perform proper experimentation and model selection to determine the best strategy for your particular problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3dd2f5-52a5-4b49-8753-276859ed2535",
   "metadata": {},
   "source": [
    "## Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7c583d-4ea7-49fa-8e24-7cbcf6b1a88b",
   "metadata": {},
   "source": [
    "The confidence interval calculated using bootstrap is a statistical technique that allows you to estimate the uncertainty or variability in a sample statistic, such as the mean, median, or other parameter, by repeatedly resampling the data with replacement. Here's a step-by-step guide on how to calculate a bootstrap confidence interval:\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - Start with your original dataset, which contains 'n' data points.\n",
    "   - Decide on the sample statistic you want to estimate, such as the mean or median.\n",
    "\n",
    "2. **Resampling**:\n",
    "   - Perform resampling (with replacement) from your original dataset to create a large number of bootstrap samples. Each bootstrap sample should have 'n' data points, just like the original dataset.\n",
    "   - Typically, you generate a large number of bootstrap samples, such as 1,000 or 10,000, to obtain a robust estimate.\n",
    "\n",
    "3. **Statistic Computation**:\n",
    "   - Calculate the sample statistic of interest (e.g., mean, median, standard deviation) for each of the bootstrap samples.\n",
    "\n",
    "4. **Construction of Confidence Interval**:\n",
    "   - Sort the computed statistics from step 3 in ascending order.\n",
    "   - Determine the desired level of confidence for your interval, typically 95% (0.95 confidence level) is used, but other confidence levels like 90% or 99% can also be chosen.\n",
    "   - To construct a two-tailed confidence interval, calculate the lower and upper percentiles of your sorted statistics based on the chosen confidence level. For a 95% confidence interval, you would select the 2.5th percentile (lower bound) and the 97.5th percentile (upper bound).\n",
    "\n",
    "5. **Reporting**:\n",
    "   - Report the computed lower and upper bounds as the confidence interval for your sample statistic.\n",
    "   - For example, if the mean of your bootstrap statistics was 50, and your 95% confidence interval ranges from 45 to 55, you would say that you are 95% confident that the true population mean falls within the interval [45, 55].\n",
    "\n",
    "The key idea behind the bootstrap method is that it estimates the sampling distribution of a statistic by simulating the process of repeatedly drawing random samples from the original data. This provides an empirical way to quantify the uncertainty associated with estimating a population parameter based on a single sample.\n",
    "\n",
    "Bootstrap confidence intervals are particularly useful when the underlying population distribution is unknown or complex, making traditional parametric methods less applicable. They are widely used for a variety of statistical applications, including estimating means, medians, standard errors, and other summary statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024ce951-9636-407f-89fe-d90fc047f509",
   "metadata": {},
   "source": [
    "## Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4612f116-b35f-4a76-87f0-1594509664f0",
   "metadata": {},
   "source": [
    "Bootstrap is a resampling technique used in statistics to estimate the sampling distribution of a statistic by repeatedly resampling from the original dataset. It's a powerful method for making inferences about population parameters or estimating the uncertainty associated with a sample statistic. Here are the steps involved in the bootstrap procedure:\n",
    "\n",
    "1. **Data Collection**:\n",
    "   - Start with your original dataset, which contains 'n' data points.\n",
    "\n",
    "2. **Resampling**:\n",
    "   - Randomly select 'n' data points from the original dataset with replacement to create a new bootstrap sample.\n",
    "   - Each data point in the original dataset has an equal probability of being selected, and some data points may be selected multiple times while others may not be selected at all.\n",
    "   - This process is repeated a large number of times (often thousands or more) to generate a collection of bootstrap samples.\n",
    "\n",
    "3. **Statistic Computation**:\n",
    "   - Calculate the sample statistic of interest (e.g., mean, median, variance, correlation) for each of the bootstrap samples.\n",
    "   - This step essentially involves applying the same statistical operation to each bootstrap sample that you want to perform on the original data.\n",
    "\n",
    "4. **Bootstrap Distribution**:\n",
    "   - The collection of statistics calculated in step 3 forms the bootstrap distribution. This distribution represents the variability of the sample statistic as it would have been observed if you had taken many random samples from the population (which you didn't).\n",
    "\n",
    "5. **Confidence Interval (Optional)**:\n",
    "   - If your goal is to estimate a confidence interval for your sample statistic, sort the bootstrap statistics in ascending order.\n",
    "   - Determine the desired level of confidence (e.g., 95% confidence interval).\n",
    "   - To construct a two-tailed confidence interval, calculate the lower and upper percentiles based on the chosen confidence level. For a 95% confidence interval, you would select the 2.5th percentile (lower bound) and the 97.5th percentile (upper bound) of the bootstrap distribution.\n",
    "\n",
    "6. **Reporting**:\n",
    "   - Report the results of your analysis, which may include:\n",
    "     - The point estimate of your sample statistic (e.g., mean).\n",
    "     - Confidence intervals, if computed.\n",
    "     - Any other relevant information about the distribution of the statistic.\n",
    "\n",
    "The key idea behind the bootstrap method is that it uses resampling from the observed data to approximate the sampling distribution of a statistic. This allows you to make inferences about population parameters or estimate the uncertainty associated with your sample statistic without relying on complex mathematical assumptions about the population distribution. Bootstrap is particularly valuable when dealing with small sample sizes or when the underlying population distribution is unknown or difficult to model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dfb8bd-2412-4fcd-bbca-f5d76e482781",
   "metadata": {},
   "source": [
    "## Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c1da4-97fd-4ef7-845d-fbdd5bf78aae",
   "metadata": {},
   "source": [
    "To estimate the 95% confidence interval for the population mean height using the bootstrap method, you can follow these steps:\n",
    "\n",
    "1. **Data Collection**:\n",
    "   - Start with your sample data, which consists of the heights of 50 trees.\n",
    "   - Sample Mean (xÌ„) = 15 meters\n",
    "   - Sample Standard Deviation (s) = 2 meters\n",
    "   - Number of Samples (n) = 50\n",
    "\n",
    "2. **Resampling**:\n",
    "   - Create a large number of bootstrap samples by randomly selecting 50 heights from the original sample with replacement. Repeat this process many times (e.g., 10,000 times).\n",
    "\n",
    "3. **Statistic Computation**:\n",
    "   - For each bootstrap sample, calculate the sample mean of the heights.\n",
    "\n",
    "4. **Bootstrap Distribution**:\n",
    "   - You now have a collection of sample means from the bootstrap samples. This collection represents the bootstrap distribution of the sample mean.\n",
    "\n",
    "5. **Confidence Interval**:\n",
    "   - To construct a 95% confidence interval, you need to find the 2.5th and 97.5th percentiles of the bootstrap distribution.\n",
    "   - The lower bound of the confidence interval corresponds to the 2.5th percentile, and the upper bound corresponds to the 97.5th percentile.\n",
    "\n",
    "6. **Calculate Confidence Interval**:\n",
    "   - Sort the collection of bootstrap sample means in ascending order.\n",
    "   - The 2.5th percentile corresponds to the index at 0.025 * (number of bootstrap samples).\n",
    "   - The 97.5th percentile corresponds to the index at 0.975 * (number of bootstrap samples).\n",
    "   - Find the values at these percentiles to obtain the lower and upper bounds of the 95% confidence interval.\n",
    "\n",
    "Let's calculate the confidence interval using Python as an example:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "sample_mean = 15  # Mean height of the sample\n",
    "sample_std = 2    # Standard deviation of the sample\n",
    "sample_size = 50  # Sample size\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_bootstrap_samples = 10000\n",
    "\n",
    "# Create an array to store bootstrap sample means\n",
    "bootstrap_sample_means = []\n",
    "\n",
    "# Perform bootstrap resampling\n",
    "for _ in range(num_bootstrap_samples):\n",
    "    bootstrap_sample = np.random.choice(sample_mean, size=sample_size, replace=True)\n",
    "    bootstrap_sample_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_sample_means.append(bootstrap_sample_mean)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "lower_bound = np.percentile(bootstrap_sample_means, 2.5)\n",
    "upper_bound = np.percentile(bootstrap_sample_means, 97.5)\n",
    "\n",
    "print(f\"95% Confidence Interval for Population Mean Height: ({lower_bound:.2f} meters, {upper_bound:.2f} meters)\")\n",
    "```\n",
    "\n",
    "This code uses random resampling with replacement to generate bootstrap samples and calculates the confidence interval for the population mean height. The result will give you the estimated range in which the true population mean height is likely to fall with 95% confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a049eed9-2f57-4124-a207-5ca5fb1c61df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
